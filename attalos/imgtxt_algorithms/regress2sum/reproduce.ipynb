{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "1. IAPR-TC12 does not reproduce on same corpus from the Wiki results\n",
    "2. The Neural Network runs 8x slower than my Interactive iPython session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from enum import Enum\n",
    "import gzip\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "# Attalos Imports\n",
    "sys.path.append('/home/kni/local-kni/_update_negsamp/kyle_update/')\n",
    "import attalos.util.log.log as l\n",
    "from attalos.dataset.dataset import Dataset\n",
    "from attalos.evaluation.evaluation import Evaluation\n",
    "import attalos.imgtxt_algorithms.util.readw2v as readw2v\n",
    "from attalos.imgtxt_algorithms.util.readw2v import initVo, readvocab\n",
    "\n",
    "# Local models\n",
    "# from mse import MSEModel\n",
    "# from negsampling import NegSamplingModel\n",
    "# from fast0tag import FastZeroTagModel\n",
    "\n",
    "import negsampling\n",
    "reload(negsampling)\n",
    "\n",
    "# Setup global objects\n",
    "logger = l.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ModelTypes(Enum):\n",
    "    mse = 1\n",
    "    negsampling = 2\n",
    "    fast0tag = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_regressor(sess, model, val_image_feats, val_one_hot, wordmatrix, k=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes a regressor and returns the precision/recall on the test data\n",
    "    Args:\n",
    "        sess: A tensorflow session\n",
    "        model_info: A dictionary containing tensorflow layers (specifically input and prediction)\n",
    "        val_image_feats: Image features to test performance on\n",
    "        val_text_tags: Text Tags to test performance on\n",
    "        w2v_model: a dictionary like object where the keys are words and the values are word vectors\n",
    "        k: Top number of items to retrieve to test precision/recall on\n",
    "        verbose: Verbose output or not\n",
    "\n",
    "    Returns:\n",
    "        evaluator: A attalos.evaluation.evaluation.Evaluation object\n",
    "    \"\"\"\n",
    "    val_pred = model.predict(sess, val_image_feats)\n",
    "    predictions = np.dot(val_pred, wordmatrix.T)\n",
    "\n",
    "    evaluator = Evaluation(val_one_hot, predictions, k)\n",
    "\n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_wordmatrix(w2v_model, dataset=None):\n",
    "    \"\"\"\n",
    "    Take a w2v dictionary and return matrix/index lookup\n",
    "    Args:\n",
    "        w2vmodel: Dictionary where keys are words and values are word vectors\n",
    "        dataset: If specified limits tags in matrix to tags in dataset\n",
    "\n",
    "    Returns:\n",
    "        w2ind: Mapping of word to index\n",
    "        wordmatrix: Numpy matrix of word vectors\n",
    "    \"\"\"\n",
    "    dataset_tags = None\n",
    "    if dataset:\n",
    "        dataset_tags = set()\n",
    "        for tags in dataset.text_feats.values():\n",
    "            dataset_tags.update(tags)\n",
    "        num_tags_in_output = len(dataset_tags.intersection(w2v_model.keys()))\n",
    "    else:\n",
    "        num_tags_in_output = len(w2v_model)\n",
    "\n",
    "    # Create word vector matrix to allow for embedding lookup\n",
    "    w2ind = {}\n",
    "    wordmatrix = np.zeros((num_tags_in_output, len(w2v_model[w2v_model.keys()[0]])), dtype=np.float32)\n",
    "    i =0\n",
    "    for word in w2v_model:\n",
    "        if dataset_tags is None or word in dataset_tags:\n",
    "            w2ind[word] = i\n",
    "            wordmatrix[i, :] = w2v_model[word]\n",
    "            i += 1\n",
    "    return w2ind, wordmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_to_onehot(dataset, w2ind):\n",
    "    \"\"\"\n",
    "    Take a dataset and prepare it for convient evaluation\n",
    "    Args:\n",
    "        dataset: attalos.dataset.dataset object\n",
    "        w2ind: a dictionary like object mapping words to their index\n",
    "\n",
    "    Returns:\n",
    "        img_feats: A matrix of image feautres\n",
    "        one_hot: A sparse matrix of one hot tags\n",
    "\n",
    "    \"\"\"\n",
    "    image_feat, tags = dataset.get_index(0)\n",
    "\n",
    "    image_feats = np.zeros((dataset.num_images, image_feat.shape[0]))\n",
    "    one_hot = dok_matrix((dataset.num_images, len(w2ind)), dtype=np.int32)\n",
    "    # Extract features and place in numpy matrix\n",
    "    for i in dataset:\n",
    "        image_feat, tags = dataset[i]\n",
    "        image_feats[i, :] = image_feat\n",
    "        for tag in tags:\n",
    "            if tag in w2ind:\n",
    "                one_hot[i, w2ind[tag]] = 1\n",
    "\n",
    "    return image_feats, csr_matrix(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir='/data/fs4/teams/attalos/features/'\n",
    "trimdata=datadir+'image/iaprtc_train_20160816_inception.hdf5'\n",
    "trtxdata=datadir+'text/iaprtc_train_20160816_text.json.gz'\n",
    "teimdata=datadir+'image/iaprtc_test_20160816_inception.hdf5'\n",
    "tetxdata=datadir+'text/iaprtc_test_20160816_text.json.gz'\n",
    "\n",
    "train_dataset = Dataset(trimdata, trtxdata, load_image_feats_in_mem=True)\n",
    "test_dataset = Dataset(teimdata, tetxdata)\n",
    "\n",
    "# Image vectors\n",
    "# Get the full vocab so we can extract only the word vectors we care about\n",
    "dataset_tags = set()\n",
    "for dataset in [train_dataset, test_dataset]:\n",
    "    for tags in dataset.text_feats.values():\n",
    "        dataset_tags.update(tags)\n",
    "        \n",
    "# Word Vectors\n",
    "w2vfile = readw2v.ReadW2V('/local_data/kni/data/vectors-phrase.bin')\n",
    "w2v_model = w2vfile.readlines(100000)\n",
    "# Require rescale of word vectors to avoid NaNs\n",
    "for word in w2v_model.keys():\n",
    "    w2v_model[word] *= 1.0\n",
    "Wd, Id = readvocab('/local_data/kni/data/vectors-phrase.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allocate GPU memory as needed (vs. allocating all the memory)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "num_epochs=1\n",
    "learning_rate=.01\n",
    "network_size=[2048,1024,200]\n",
    "model_input_path = None\n",
    "model_output_path = None\n",
    "verbose=True\n",
    "model_type=ModelTypes.negsampling\n",
    "max_pos=5\n",
    "max_neg=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def train_model(train_dataset,\n",
    "#                 test_dataset,\n",
    "#                 w2v_model,\n",
    "#                 batch_size=128,\n",
    "#                 num_epochs=200,\n",
    "#                 learning_rate=1.001,\n",
    "#                 network_size=[200,200],\n",
    "#                 model_input_path = None,\n",
    "#                 model_output_path = None,\n",
    "#                 verbose=True,\n",
    "#                 model_type=ModelTypes.negsampling,\n",
    "#                 max_pos=5,\n",
    "#                 max_neg=10):\n",
    "\"\"\"\n",
    "Train a regression model to map image features into the word vector space\n",
    "Args:\n",
    "    train_dataset: Training attalos.dataset.dataset object\n",
    "    test_dataset: Test attalos.dataset.dataset object\n",
    "    w2v_model: A dictionary like object where the keys are words and the values are word vectors\n",
    "    batch_size: Batch size to use for training\n",
    "    num_epochs: Number of epochs to train for\n",
    "    learning_rate: The learning rate for the network\n",
    "    network_size: A list defining the size of each layer of the neural network\n",
    "    model_input_path: Path to a file containing initial weights\n",
    "    model_output_path: Path to save final weights\n",
    "    verbose: Amounto fdebug information to output\n",
    "Returns:\n",
    "\"\"\"\n",
    "# Get validation data\n",
    "#  Extract features from first image\n",
    "image_feats, tags = test_dataset.get_index(0)\n",
    "# Get shape and initialize numpy matrix\n",
    "image_feat_size = image_feats.shape[0]\n",
    "\n",
    "\n",
    "# Turn w2v dictionary into a matrix\n",
    "w2ind, word_matrix = create_wordmatrix(w2v_model)\n",
    "val_w2ind, val_word_matrix = create_wordmatrix(w2v_model, test_dataset)\n",
    "\n",
    "# Precompute onehot representation for evaluation\n",
    "val_image_feats, val_one_hot = dataset_to_onehot(test_dataset, val_w2ind)\n",
    "\n",
    "\n",
    "# Setup data structures for negative sampling\n",
    "if model_type == ModelTypes.negsampling or model_type == ModelTypes.fast0tag:\n",
    "    word_counts = np.zeros(word_matrix.shape[0])\n",
    "    for item_id in train_dataset:\n",
    "        _, tags = train_dataset[item_id]\n",
    "        for tag in tags:\n",
    "            if tag in w2ind:\n",
    "                word_counts[w2ind[tag]] += 1\n",
    "    labelpdf = word_counts / word_counts.sum()\n",
    "    vocabsize = word_matrix.shape[0]\n",
    "    def negsamp(ignored_inds, num2samp):\n",
    "        # Negative sampler that takes in indicies\n",
    "\n",
    "        # Create new probability vector excluding positive samples\n",
    "        nlabelpdf = np.copy(labelpdf)\n",
    "        nlabelpdf[ignored_inds] = 0\n",
    "        nlabelpdf /= nlabelpdf.sum()\n",
    "\n",
    "        return np.random.choice(vocabsize, size=num2samp, p=nlabelpdf)\n",
    "\n",
    "# Time to start building our graph\n",
    "tf.Graph().as_default()\n",
    "# Build regressor\n",
    "print model_type\n",
    "if model_type == ModelTypes.mse:\n",
    "    logger.info('Building regressor with mean square error loss')\n",
    "    model = MSEModel(image_feat_size,\n",
    "                                 word_matrix,\n",
    "                                learning_rate=learning_rate,\n",
    "                                hidden_units=network_size,\n",
    "                                use_batch_norm=True)\n",
    "# elif model_type == ModelTypes.negsampling:\n",
    "elif True:\n",
    "    logger.info('Building regressor with negative sampling loss')\n",
    "    model = negsampling.NegSamplingModel(image_feat_size,\n",
    "                                         word_matrix,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         hidden_units=network_size,\n",
    "                                         optim_words=True,\n",
    "                                         use_batch_norm=True)\n",
    "    print \"Negative sampling model created\"\n",
    "elif model_type == ModelTypes.fast0tag:\n",
    "    logger.info('Building model with fast zero tag loss')\n",
    "    model = FastZeroTagModel(image_feat_size,\n",
    "                                word_matrix,\n",
    "                                learning_rate=learning_rate,\n",
    "                                hidden_units=network_size,\n",
    "                                use_batch_norm=True)\n",
    "# Initialize model\n",
    "model.initialize_model(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optionally restore saved model\n",
    "if model_input_path:\n",
    "    model.load(sess, model_input_path)\n",
    "\n",
    "# Reuse space for each iteration\n",
    "pos_word_ids = np.ones((batch_size, max_pos), dtype=np.int32)\n",
    "neg_word_ids = np.ones((batch_size, max_neg), dtype=np.int32)\n",
    "performance = []\n",
    "for epoch in range(num_epochs):\n",
    "    batch_time_total = 0\n",
    "    run_time_total = 0\n",
    "\n",
    "    loss = None\n",
    "    for batch in range(int(train_dataset.num_images/batch_size)):\n",
    "        batch_time = time.time()\n",
    "        # Get raw data\n",
    "        image_feats, text_tags = train_dataset.get_next_batch(batch_size)\n",
    "\n",
    "        # Generate positive examples\n",
    "        pos_word_ids.fill(-1)\n",
    "        for i, tags in enumerate(text_tags):\n",
    "            j = 0\n",
    "            for tag in tags:\n",
    "                if tag in w2ind and j < max_pos:\n",
    "                    pos_word_ids[i, j] = w2ind[tag]\n",
    "                    j += 1\n",
    "\n",
    "        if model_type == ModelTypes.negsampling or model_type == ModelTypes.fast0tag:\n",
    "            neg_word_ids.fill(-1)\n",
    "            for i in range(neg_word_ids.shape[0]):\n",
    "                neg_word_ids[i] = negsamp(pos_word_ids, max_neg)\n",
    "\n",
    "        batch_time = time.time() - batch_time\n",
    "        batch_time_total += batch_time\n",
    "\n",
    "        run_time = time.time()\n",
    "        if model_type == ModelTypes.mse:\n",
    "            loss = model.fit(sess, image_feats, pos_word_ids)\n",
    "        elif model_type == ModelTypes.negsampling or model_type == ModelTypes.fast0tag:\n",
    "            loss = model.fit(sess, image_feats,pos_word_ids, neg_word_ids=neg_word_ids)\n",
    "        run_time = time.time() - run_time\n",
    "        run_time_total += run_time\n",
    "\n",
    "    if verbose:\n",
    "        eval_time = time.time()\n",
    "        evaluator = evaluate_regressor(sess, model, val_image_feats, val_one_hot, val_word_matrix, verbose=False)\n",
    "        performance.append(evaluator.evaluate())\n",
    "        eval_time = time.time() - eval_time\n",
    "        # Evaluate accuracy\n",
    "        #print('Epoch {}: Loss: {} Timing: {} {} {}'.format(epoch, loss, batch_time_total, run_time_total, eval_time))\n",
    "        logger.debug('Epoch {}: Loss: {} Perf: {} {} {}'.format(epoch, loss, *performance[-1]))\n",
    "\n",
    "if model_output_path:\n",
    "    model.save(sess, model_output_path)\n",
    "\n",
    "# return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = image_feats\n",
    "y = pos_word_ids\n",
    "neg_ids = neg_word_ids\n",
    "w2v = word_matrix\n",
    "\n",
    "pvecs = np.zeros((y.shape[0], y.shape[1], w2v.shape[1]))                                                           \n",
    "nvecs = np.zeros((neg_ids.shape[0], neg_ids.shape[1], w2v.shape[1]))      \n",
    "for i, ids in enumerate(y):                                                                                             \n",
    "    pvecs[i] = w2v[ids] \n",
    "for i, ids in enumerate(neg_ids): \n",
    "    nvecs[i] = w2v[ids]\n",
    "pvecs=pvecs.transpose((1,0,2))\n",
    "nvecs=nvecs.transpose((1,0,2))\n",
    "\n",
    "_, loss, preds = sess.run([model.model_info['optimizer'], model.model_info['loss'], model.model_info['prediction']],\n",
    "                          feed_dict={ model.model_info['input']: x,\n",
    "                                     model.model_info['pos_vecs']: pvecs,\n",
    "                                     model.model_info['neg_vecs']: nvecs  \n",
    "                                    })                                                                                 \n",
    "model.updatewords(y, neg_ids, preds) \n",
    "    \n",
    "zip(y,neg_ids)[0]\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
